{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from typing import List, Tuple  # for type hints\n",
    "\n",
    "import numpy as np  # for manipulating arrays\n",
    "import pandas as pd  # for manipulating data in dataframes\n",
    "import plotly.express as px  # for plots\n",
    "import random  # for generating run IDs\n",
    "from sklearn.model_selection import train_test_split  # for splitting train & test data\n",
    "import torch  # for matrix optimization\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import os\n",
    "import datasets\n",
    "from livelossplot import PlotLosses\n",
    "from model_factory import model_factory\n",
    "import itertools\n",
    "from tl_models import LinearTransformationModel, ElementwiseProductModel, StackWiseProductModel\n",
    "from sentence_transformers.util import pairwise_angle_sim\n",
    "\n",
    "from mteb import MTEB\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\n",
    "    \"STS12\",\n",
    "    \"STSBenchmark\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_MODELS = os.listdir(\"data\")\n",
    "BASIC_MODELS.remove(\"sentences\")\n",
    "BASIC_MODELS.remove(\"cohere-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading task: STS12\n",
      "Split: train\n",
      "Loading task: STSBenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3120538/1480070816.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['score'] = normalize(train_df['score'], train_df['score'].min(), train_df['score'].max())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train\n",
      "Split: validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3120538/1480070816.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['score'] = normalize(train_df['score'], train_df['score'].min(), train_df['score'].max())\n"
     ]
    }
   ],
   "source": [
    "def load_dataset_for_task(task_name: str):\n",
    "    # Note, we load all splits except the test set\n",
    "    mteb = MTEB(tasks=[task_name], task_langs=[\"en\"])\n",
    "    task = mteb.tasks[0]\n",
    "    task.load_data()\n",
    "    assert task.data_loaded, \"Data was not loaded\"\n",
    "    lis_dfs = []\n",
    "    for split in task.dataset:\n",
    "        if split == 'test':\n",
    "            continue\n",
    "        print(f\"Split: {split}\")\n",
    "        lis_dfs.append(task.dataset[split].to_pandas())\n",
    "    return pd.concat(lis_dfs)\n",
    "\n",
    "def process_dataset(train_df: datasets.Dataset) -> pd.DataFrame:\n",
    "    train_df = train_df[['sentence1', 'sentence2', 'score']]\n",
    "    def normalize(x, min_score, max_score): # Norm between 0 and 1\n",
    "        zero_one = (x - min_score) / (max_score - min_score)\n",
    "        scaled_down = zero_one \n",
    "        return scaled_down \n",
    "    train_df['score'] = normalize(train_df['score'], train_df['score'].min(), train_df['score'].max())\n",
    "    return train_df\n",
    "\n",
    "dfs = {}\n",
    "for task_name in TASKS:\n",
    "    print(\"Loading task:\", task_name)\n",
    "    dataset_df = load_dataset_for_task(task_name)\n",
    "    df_task = process_dataset(dataset_df)\n",
    "    dfs[task_name] = df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityModel(torch.nn.Module):\n",
    "    def __init__(self, transformation_model: torch.nn.Module):\n",
    "        super(CosineSimilarityModel, self).__init__()\n",
    "        self.transformation_model = transformation_model\n",
    "    \n",
    "    def forward(self, embeddings1: torch.Tensor, embeddings2: torch.Tensor):\n",
    "        embedding1_custom = self.transformation_model(embeddings1)\n",
    "        embedding2_custom = self.transformation_model(embeddings2)\n",
    "        return torch.nn.functional.cosine_similarity(embedding1_custom, embedding2_custom, dim=1)\n",
    "\n",
    "class AnglEModel(torch.nn.Module):\n",
    "    def __init__(self, transformation_model: torch.nn.Module, scale: float = 20.0):\n",
    "        super(AnglEModel, self).__init__()\n",
    "        self.transformation_model = transformation_model\n",
    "        self.scale = scale\n",
    "    \n",
    "    def forward(self, embeddings1: torch.Tensor, embeddings2: torch.Tensor):\n",
    "        scores = pairwise_angle_sim(self.transformation_model(embeddings1), self.transformation_model(embeddings2))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(cosine_similarity: torch.Tensor, scores: torch.Tensor):\n",
    "    lossF = torch.nn.MSELoss()\n",
    "    return lossF(cosine_similarity, scores)\n",
    "\n",
    "def spearman_correlation(cosine_similarity: torch.Tensor, scores: torch.Tensor):\n",
    "    spearman_corr = torchmetrics.SpearmanCorrCoef().to(device)\n",
    "    return spearman_corr(cosine_similarity, scores)\n",
    "\n",
    "def pearson_correlation(cosine_similarity: torch.Tensor, scores: torch.Tensor):\n",
    "    pearson_corr = torchmetrics.PearsonCorrCoef().to(device)\n",
    "    return pearson_corr(cosine_similarity, scores)\n",
    "\n",
    "def angle_loss(scores: torch.Tensor, labels: torch.Tensor, scale: float = 20.0):\n",
    "    scores = scores * scale\n",
    "    scores = scores[:, None] - scores[None, :]\n",
    "\n",
    "    labels = labels[:, None] < labels[None, :]\n",
    "    labels = labels.float()\n",
    "\n",
    "    scores = scores - (1 - labels) * 1e12\n",
    "\n",
    "    scores = torch.cat((torch.zeros(1).to(device), scores.view(-1)), dim=0)\n",
    "    loss = torch.logsumexp(scores, dim=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, training_loader, optimizer, loss_function = None):\n",
    "    running_loss = 0.\n",
    "\n",
    "    for X1, X2, Y in training_loader:\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        cosine_sim = model(X1, X2)\n",
    "        loss = loss_function(cosine_sim, Y)\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    average_loss = running_loss / len(training_loader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model angle$flag-embedding$gist$gte-large$llmrails$mixed-bread$voyage already trained\n",
      "Model angle$cohere$flag-embedding$gist$gte-large$llmrails$mixed-bread already trained\n",
      "Model angle$cohere$flag-embedding$gist$gte-large$mixed-bread$voyage already trained\n",
      "Model angle$cohere$flag-embedding$gist$llmrails$mixed-bread$voyage already trained\n",
      "Model angle$cohere$flag-embedding$gte-large$llmrails$mixed-bread$voyage already trained\n",
      "Model angle$cohere$flag-embedding$gist$gte-large$llmrails$voyage already trained\n",
      "Model angle$cohere$gist$gte-large$llmrails$mixed-bread$voyage already trained\n",
      "Model cohere$flag-embedding$gist$gte-large$llmrails$mixed-bread$voyage already trained\n",
      "Model angle$cohere$flag-embedding$gist$gte-large$llmrails$mixed-bread$voyage already trained\n"
     ]
    }
   ],
   "source": [
    "def get_cosine_similarity(a: np.array, b: np.array):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# generating transfer learning model\n",
    "for r in range(7, len(BASIC_MODELS) + 1):\n",
    "    combinations_object = itertools.combinations(BASIC_MODELS, r)\n",
    "    combinations_list = [sorted(list(combination)) for combination in combinations_object]\n",
    "    \n",
    "    # generating model for each combination\n",
    "    for combination in combinations_list:\n",
    "        model_name = \"$\".join(combination)\n",
    "        \n",
    "        # check if the model is already trained\n",
    "        if os.path.exists(f\"pca_tl/{model_name}/final_model.pth\"):\n",
    "            print(f\"Model {model_name} already trained\")\n",
    "            continue\n",
    "        \n",
    "        model_name += \"-pca\"\n",
    "        \n",
    "        # load a copy of the dataframes\n",
    "        dfs_copy = dfs.copy()\n",
    "        \n",
    "        # generating emebeddings for each task\n",
    "        for task_name in TASKS:\n",
    "            df = dfs_copy[task_name]\n",
    "            # generating embeddings\n",
    "            model = model_factory(model_name, task_name)\n",
    "            for column in ['sentence1', 'sentence2']:\n",
    "                embs = model.encode(df[column].tolist())\n",
    "                df[f\"{column}_embedding\"] = list(embs)\n",
    "        \n",
    "            # computing cosine similarity\n",
    "            df[\"cosine_similarity\"] = df.apply(lambda x: get_cosine_similarity(x['sentence1_embedding'], x['sentence2_embedding']), axis=1)\n",
    "\n",
    "        # Combine all the dataframes\n",
    "        df = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "        # Shuffle the data\n",
    "        df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "        # Split the data\n",
    "        df_train, df_val = train_test_split(df, test_size=0.2, random_state=seed)\n",
    "\n",
    "        # Reset the index\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "        df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "        def get_tensors(df):\n",
    "            df_x1 = np.stack(df['sentence1_embedding'].values)\n",
    "            df_x2 = np.stack(df['sentence2_embedding'].values)\n",
    "            df_y = df['score'].values\n",
    "\n",
    "            X1 = torch.from_numpy(df_x1).float()\n",
    "            X2 = torch.from_numpy(df_x2).float()\n",
    "            Y = torch.from_numpy(df_y).float()\n",
    "            return X1, X2, Y\n",
    "            \n",
    "        X1_train, X2_train, Y_train = get_tensors(df_train)\n",
    "        X1_val, X2_val, Y_val = get_tensors(df_val)\n",
    "\n",
    "        # Move everything to the device\n",
    "        X1_train = X1_train.to(device)\n",
    "        X2_train = X2_train.to(device)\n",
    "        Y_train = Y_train.to(device)\n",
    "\n",
    "        X1_val = X1_val.to(device)\n",
    "        X2_val = X2_val.to(device)\n",
    "        Y_val = Y_val.to(device)\n",
    "\n",
    "        # train model\n",
    "        model_name = model_name.replace(\"-pca\", \"\")\n",
    "        output_dir = f\"pca_tl/{model_name}\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Hyperparameters\n",
    "        max_epochs = 3_000\n",
    "        lr = 1e-4 # 1e-5 is the smoothest\n",
    "        batch_size = 300\n",
    "        momentum = 0.9\n",
    "        \n",
    "        loss_function = angle_loss\n",
    "        \n",
    "        transformation_model = LinearTransformationModel(1024, 1024, dropout_rate=0.4)\n",
    "        \n",
    "        model = AnglEModel(transformation_model)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "        # Data Loader\n",
    "        train_dataset = torch.utils.data.TensorDataset(X1_train, X2_train, Y_train)\n",
    "        training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        # Keep track of losses\n",
    "        plotlosses = PlotLosses()\n",
    "        \n",
    "        model.train()\n",
    "        best_accuracy = 0.9 # Should be better then this\n",
    "        epochs_saved = []\n",
    "        for epoch_num in range(max_epochs):\n",
    "            epoch_loss = train_one_epoch(model, training_loader, optimizer, loss_function)\n",
    "\n",
    "            # Additional metrics for performance tracking\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Generate the validation loss\n",
    "                val_predictions = model(X1_val, X2_val)\n",
    "                val_loss = loss_function(val_predictions, Y_val).item()\n",
    "\n",
    "                # Compute the correlations\n",
    "                train_spearman = spearman_correlation(model(X1_train, X2_train), Y_train).item()\n",
    "                val_spearman = spearman_correlation(val_predictions, Y_val).item()\n",
    "\n",
    "                # Save locally if it is the best\n",
    "                if val_spearman > best_accuracy: # Only save if it does not change much\n",
    "                    epochs_saved.append(epoch_num)\n",
    "                    best_accuracy = val_spearman\n",
    "                    torch.save({'transformation_model': model.transformation_model}, f\"{output_dir}/best_model.pth\")\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            plotlosses.update({'loss': epoch_loss, 'val_loss': val_loss, 'acc': train_spearman, 'val_acc': val_spearman})\n",
    "            plotlosses.send()\n",
    "        \n",
    "        torch.save({\"transformation_model\": model.transformation_model}, f\"{output_dir}/final_model.pth\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Transfer Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANFER_LEARNING_MODELS = os.listdir(\"pca_tl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from run_utils import run_on_tasks \n",
    "\n",
    "for model in TRANFER_LEARNING_MODELS:\n",
    "    model_name = model + \"-transfer\"\n",
    "    run_on_tasks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
