{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.1.2",
  "test": {
    "cos_sim": {
      "accuracy": 0.8759015318590928,
      "accuracy_threshold": 0.8128966030783118,
      "ap": 0.7859562920324268,
      "f1": 0.7171295158515606,
      "f1_threshold": 0.7897252312590403,
      "precision": 0.671113155473781,
      "recall": 0.7699208443271768
    },
    "dot": {
      "accuracy": 0.8759015318590928,
      "accuracy_threshold": 0.8128966030783118,
      "ap": 0.7859562920324268,
      "f1": 0.7171295158515606,
      "f1_threshold": 0.7897252312590404,
      "precision": 0.671113155473781,
      "recall": 0.7699208443271768
    },
    "euclidean": {
      "accuracy": 0.8759015318590928,
      "accuracy_threshold": 0.6117244417426069,
      "ap": 0.7859562920324268,
      "f1": 0.7171295158515606,
      "f1_threshold": 0.6484979083455606,
      "precision": 0.671113155473781,
      "recall": 0.7699208443271768
    },
    "evaluation_time": 16.1,
    "manhattan": {
      "accuracy": 0.87578232103475,
      "accuracy_threshold": 30.721282082408187,
      "ap": 0.785385075819276,
      "f1": 0.7162917857586678,
      "f1_threshold": 32.260513140864745,
      "precision": 0.6770025839793282,
      "recall": 0.7604221635883905
    },
    "max": {
      "accuracy": 0.8759015318590928,
      "ap": 0.7859562920324268,
      "f1": 0.7171295158515606
    }
  }
}