{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.1.2",
  "test": {
    "cos_sim": {
      "accuracy": 0.8778685104607499,
      "accuracy_threshold": 0.7347652056296827,
      "ap": 0.7873592998632704,
      "f1": 0.7186914720938944,
      "f1_threshold": 0.6966146361246629,
      "precision": 0.6821521687603698,
      "recall": 0.7593667546174142
    },
    "dot": {
      "accuracy": 0.8612386004649222,
      "accuracy_threshold": 0.3785411560413847,
      "ap": 0.7403867812583314,
      "f1": 0.6815944881889763,
      "f1_threshold": 0.3481142267050171,
      "precision": 0.6385431074227754,
      "recall": 0.7308707124010554
    },
    "euclidean": {
      "accuracy": 0.8780473266972642,
      "accuracy_threshold": 0.5223226144275936,
      "ap": 0.7920400059266999,
      "f1": 0.7227001985440107,
      "f1_threshold": 0.5373774598524892,
      "precision": 0.7250996015936255,
      "recall": 0.7203166226912929
    },
    "evaluation_time": 2.44,
    "manhattan": {
      "accuracy": 0.8694641473445789,
      "accuracy_threshold": 11.267918953659475,
      "ap": 0.7692081715554228,
      "f1": 0.70390625,
      "f1_threshold": 11.815642752655986,
      "precision": 0.6948586118251928,
      "recall": 0.7131926121372032
    },
    "max": {
      "accuracy": 0.8780473266972642,
      "ap": 0.7920400059266999,
      "f1": 0.7227001985440107
    }
  }
}