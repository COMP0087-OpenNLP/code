{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.1.2",
  "test": {
    "cos_sim": {
      "accuracy": 0.8736365261965786,
      "accuracy_threshold": 0.7519770145516599,
      "ap": 0.7799208011739678,
      "f1": 0.7136244213686975,
      "f1_threshold": 0.7060564479163924,
      "precision": 0.6785629312395908,
      "recall": 0.7525065963060686
    },
    "dot": {
      "accuracy": 0.8572450378494367,
      "accuracy_threshold": 0.4528834789727692,
      "ap": 0.7292954260630948,
      "f1": 0.6733083193168585,
      "f1_threshold": 0.42438012212823195,
      "precision": 0.6605737496826606,
      "recall": 0.6865435356200528
    },
    "euclidean": {
      "accuracy": 0.8745902127913214,
      "accuracy_threshold": 0.5493700129835825,
      "ap": 0.7847196885161672,
      "f1": 0.717825537294564,
      "f1_threshold": 0.585782481621522,
      "precision": 0.6890776699029126,
      "recall": 0.7490765171503958
    },
    "evaluation_time": 1.55,
    "manhattan": {
      "accuracy": 0.8679144066281218,
      "accuracy_threshold": 9.994927323999988,
      "ap": 0.7636673166138257,
      "f1": 0.698586527441742,
      "f1_threshold": 10.721138671229166,
      "precision": 0.6751169086881614,
      "recall": 0.7237467018469657
    },
    "max": {
      "accuracy": 0.8745902127913214,
      "ap": 0.7847196885161672,
      "f1": 0.717825537294564
    }
  }
}