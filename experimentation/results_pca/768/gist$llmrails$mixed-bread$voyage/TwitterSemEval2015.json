{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.1.2",
  "test": {
    "cos_sim": {
      "accuracy": 0.8759015318590928,
      "accuracy_threshold": 0.7454409331842611,
      "ap": 0.7876192358099949,
      "f1": 0.7199284162086156,
      "f1_threshold": 0.707596424841741,
      "precision": 0.6982395239275974,
      "recall": 0.7430079155672823
    },
    "dot": {
      "accuracy": 0.8612982058770936,
      "accuracy_threshold": 0.42776530882480485,
      "ap": 0.7400223112443037,
      "f1": 0.681516217272372,
      "f1_threshold": 0.4054965454282884,
      "precision": 0.673012606122974,
      "recall": 0.6902374670184697
    },
    "euclidean": {
      "accuracy": 0.8770936401025213,
      "accuracy_threshold": 0.5461198324965952,
      "ap": 0.7914298981018317,
      "f1": 0.7239465051728489,
      "f1_threshold": 0.5802654604544132,
      "precision": 0.6936653771760155,
      "recall": 0.7569920844327177
    },
    "evaluation_time": 1.47,
    "manhattan": {
      "accuracy": 0.8697025689932646,
      "accuracy_threshold": 10.213473486381735,
      "ap": 0.7708443140219925,
      "f1": 0.7026959247648903,
      "f1_threshold": 11.076181638677763,
      "precision": 0.6695340501792114,
      "recall": 0.7393139841688654
    },
    "max": {
      "accuracy": 0.8770936401025213,
      "ap": 0.7914298981018317,
      "f1": 0.7239465051728489
    }
  }
}