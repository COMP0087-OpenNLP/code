{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.1.2",
  "test": {
    "cos_sim": {
      "accuracy": 0.8728616558383501,
      "accuracy_threshold": 0.750924508389141,
      "ap": 0.7767020509132642,
      "f1": 0.7109122545364157,
      "f1_threshold": 0.7106737841545052,
      "precision": 0.6719924812030075,
      "recall": 0.7546174142480211
    },
    "dot": {
      "accuracy": 0.8524766048757227,
      "accuracy_threshold": 0.3791695016664728,
      "ap": 0.7157328774987559,
      "f1": 0.6568062170970168,
      "f1_threshold": 0.3431474931236615,
      "precision": 0.6255969436485196,
      "recall": 0.6912928759894459
    },
    "euclidean": {
      "accuracy": 0.8743517911426357,
      "accuracy_threshold": 0.49004037927729627,
      "ap": 0.7811152291194476,
      "f1": 0.714498510427011,
      "f1_threshold": 0.5277227154175044,
      "precision": 0.6746366619784341,
      "recall": 0.7593667546174142
    },
    "evaluation_time": 0.87,
    "manhattan": {
      "accuracy": 0.8651129522560649,
      "accuracy_threshold": 9.100683428002025,
      "ap": 0.7571313547888976,
      "f1": 0.6937354988399073,
      "f1_threshold": 9.57899216790771,
      "precision": 0.6781754032258065,
      "recall": 0.7100263852242744
    },
    "max": {
      "accuracy": 0.8743517911426357,
      "ap": 0.7811152291194476,
      "f1": 0.714498510427011
    }
  }
}