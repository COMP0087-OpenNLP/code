{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.1.2",
  "test": {
    "cos_sim": {
      "accuracy": 0.8717887584192645,
      "accuracy_threshold": 0.757410334710569,
      "ap": 0.7745403896221756,
      "f1": 0.7089230769230769,
      "f1_threshold": 0.7087945948542491,
      "precision": 0.6643598615916955,
      "recall": 0.7598944591029023
    },
    "dot": {
      "accuracy": 0.8555164808964654,
      "accuracy_threshold": 0.2831399061024802,
      "ap": 0.7247861666163782,
      "f1": 0.6658572122389509,
      "f1_threshold": 0.26122181022356394,
      "precision": 0.6167341430499326,
      "recall": 0.7234828496042216
    },
    "euclidean": {
      "accuracy": 0.8728020504261786,
      "accuracy_threshold": 0.43261610971327036,
      "ap": 0.7769104072435474,
      "f1": 0.7114076735531888,
      "f1_threshold": 0.4554432715206783,
      "precision": 0.6924806395203598,
      "recall": 0.7313984168865435
    },
    "evaluation_time": 1.18,
    "manhattan": {
      "accuracy": 0.8621326816474936,
      "accuracy_threshold": 8.909815036406457,
      "ap": 0.7466324005269669,
      "f1": 0.6833939393939393,
      "f1_threshold": 9.718234162552568,
      "precision": 0.6320627802690583,
      "recall": 0.7437994722955145
    },
    "max": {
      "accuracy": 0.8728020504261786,
      "ap": 0.7769104072435474,
      "f1": 0.7114076735531888
    }
  }
}