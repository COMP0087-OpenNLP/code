{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.1.2",
  "test": {
    "cos_sim": {
      "accuracy": 0.8788221970554927,
      "accuracy_threshold": 0.7430801576878934,
      "ap": 0.7921481701075112,
      "f1": 0.7226192022866053,
      "f1_threshold": 0.7107845461791599,
      "precision": 0.7117993345277707,
      "recall": 0.7337730870712401
    },
    "dot": {
      "accuracy": 0.8590928056267509,
      "accuracy_threshold": 0.3634399800761541,
      "ap": 0.7359830360692096,
      "f1": 0.6786505145239025,
      "f1_threshold": 0.341732633658245,
      "precision": 0.6701826601492153,
      "recall": 0.6873350923482849
    },
    "euclidean": {
      "accuracy": 0.8791202241163497,
      "accuracy_threshold": 0.4866565911449524,
      "ap": 0.7951629703416543,
      "f1": 0.7253263707571802,
      "f1_threshold": 0.5243446563877463,
      "precision": 0.7178294573643411,
      "recall": 0.732981530343008
    },
    "evaluation_time": 1.97,
    "manhattan": {
      "accuracy": 0.8759015318590928,
      "accuracy_threshold": 8.311111710976048,
      "ap": 0.7880816631293653,
      "f1": 0.7210097291611886,
      "f1_threshold": 8.759314241929264,
      "precision": 0.7185534591194969,
      "recall": 0.7234828496042216
    },
    "max": {
      "accuracy": 0.8791202241163497,
      "ap": 0.7951629703416543,
      "f1": 0.7253263707571802
    }
  }
}